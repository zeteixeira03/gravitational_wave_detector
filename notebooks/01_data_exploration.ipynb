{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "This file's purpose is to aid in visualizing the data that was used to train and validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last executed: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "from __future__ import annotations\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Some fool-proofing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"src\").exists() and PROJECT_ROOT != PROJECT_ROOT.parent:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Import utilities\n",
    "from data.g2net import find_dataset_dir, load_labels, load_sample\n",
    "\n",
    "# Resolve dataset directory automatically\n",
    "DATASET_DIR = find_dataset_dir(PROJECT_ROOT)\n",
    "\n",
    "# Load labels\n",
    "train_labels_df = load_labels(DATASET_DIR)\n",
    "\n",
    "# Constants for plotting / spectrograms\n",
    "FS = 2048                          # sampling rate (Hz)\n",
    "N = 4096                           # samples per detector\n",
    "T = np.arange(N) / FS              # time axis (s)\n",
    "DETECTORS = [\"Hanford (H1)\", \"Livingston (L1)\", \"Virgo (V1)\"]\n",
    "\n",
    "# Quick sanity prints\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATASET_DIR:  \", DATASET_DIR)\n",
    "print(\"Labels file:  \", (DATASET_DIR / \"training_labels.csv\"))\n",
    "print(\"train_df:     \", train_labels_df.shape, \"| target mean:\", float(train_labels_df[\"target\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We already have:\n",
    "# train_labels_df = load_labels(DATASET_DIR)\n",
    "# columns: [\"id\", \"target\"]\n",
    "\n",
    "df = train_labels_df.copy()\n",
    "\n",
    "# 1) Split into train and temp (val+test)\n",
    "train_ids, temp_ids = train_test_split(\n",
    "    df[\"id\"],\n",
    "    test_size=0.30,             # 30% will be val+test\n",
    "    stratify=df[\"target\"],      # keep class balance\n",
    "    random_state=42,            # reproducible\n",
    ")\n",
    "\n",
    "train_df = df[df[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "temp_df  = df[df[\"id\"].isin(temp_ids)].reset_index(drop=True)\n",
    "\n",
    "# 2) Split temp into val and test (each ~15% of total)\n",
    "val_ids, test_ids = train_test_split(\n",
    "    temp_df[\"id\"],\n",
    "    test_size=0.50,             # half of temp -> 15% overall\n",
    "    stratify=temp_df[\"target\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "val_df  = df[df[\"id\"].isin(val_ids)].reset_index(drop=True)\n",
    "test_df = df[df[\"id\"].isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "# 3) Print how many instances are in each split\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Val samples:   {len(val_df)}\")\n",
    "print(f\"Test samples:  {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Load Training Labels\n",
    "\n",
    "The model we shall build will perform supervised learning on the G2Net Competition dataset. The labels for each training instance are stored in `training_labels.csv`, whose first 5 instances are shown below.\n",
    "\n",
    "\n",
    "`training_labels.csv` contains two columns:\n",
    "   - `id`: a unique identifier for each 2-second sample. It does *not* hold any information about the physical nature of the system (masses, spin, sky location, etc.)\n",
    "   - `target`: the label (0 = noise-only, 1 = noise + simulated GW signal)\n",
    "\n",
    " The characters in the `id` are hexadecimal (0-9, a-f) and act as a unique key.\n",
    " From the filename alone, the only information we can infer is:\n",
    "   1) which sample it is (unique id),\n",
    "   2) which subfolders it belongs to (first 3 characters),\n",
    "   3) whether it is training or test data (by whether it is in train/ or test/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We now check the first sample from `train_labels_df` and select its `id` and `target` values. We then use `load_sample` (defined in `g2net.py`) to retrieve the training sample that specific `id` points to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = train_labels_df.iloc[0]['id']\n",
    "target_value = train_labels_df.loc[train_labels_df['id'] == sample_id, 'target'].item()\n",
    "sample = load_sample(sample_id)\n",
    "\n",
    "print(f\"Sample ID: \\n {sample_id}\")\n",
    "print(f\"Sample: \\n {sample} -> notice it has 3 arrays, one for each detector\")\n",
    "print(f\"Sample shape: \\n {sample.shape} -> 3 detectors each with a 2s time series at 2048 Hz (4096 points)\")\n",
    "print(\"Is black hole?\", \"Yes!\" if int(target_value) == 1 else \"No :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "From the print statements above:\n",
    "\n",
    "- 00000e74ad is the first file listed in `train_labels_df`. This, however, does not mean it is also the first item on \\train\\0\\0\\0, as the training labels file has been shuffled.\n",
    "- the sample itself is a `numpy` array, with 3 other arrays contained within. Each of those arrays represents one of the LIGO detectors (Hanford, Livingston, and Virgo, respectively), each holding 4096 float values representing the 2-second signal itself.\n",
    "\n",
    "The values inside `sample` themselves represent the *fractional change in arm length* the detector would experience due to a passing gravitational wave:\n",
    "\n",
    "$$\n",
    "h(t) = \\frac{\\Delta L}{L},\n",
    "$$\n",
    "\n",
    "where the sign indicates the direction of change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df['target'].value_counts().rename({0: \"noise\", 1: \"signal\"})\n",
    "train_labels_df['target'].value_counts(normalize=True).rename({0: \"noise\", 1: \"signal\"})\n",
    "\n",
    "# plus add a bar graph to show its not unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_simple_features(x):\n",
    "    # x: (3, 4096)\n",
    "    feats = {}\n",
    "    for i, det_name in enumerate(DETECTORS):\n",
    "        sig = x[i]\n",
    "        feats[f\"{det_name}_rms\"] = np.sqrt(np.mean(sig**2))\n",
    "        feats[f\"{det_name}_max\"] = np.max(np.abs(sig))\n",
    "    return feats\n",
    "\n",
    "# example: 100 samples per class\n",
    "n_samples = 100\n",
    "features = []\n",
    "\n",
    "for label in [0, 1]:\n",
    "    rows = train_labels_df[train_labels_df[\"target\"] == label].sample(n_samples, random_state=0)\n",
    "    for _, row in rows.iterrows():\n",
    "        x = load_sample(row[\"id\"])\n",
    "        feats = compute_simple_features(x)\n",
    "        feats[\"target\"] = label\n",
    "        features.append(feats)\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Time-Series Evolution\n",
    "\n",
    "In this section we explore one `sample` in more detail by plotting the strain vs time for all three detectors (H1, L1, V1).\n",
    "\n",
    "For each detector we annotate:\n",
    "\n",
    "1) **Peak strain time**  \n",
    "   We mark the time index where the absolute strain reaches its maximum within the 2-second window.  \n",
    "   In an ideal high-SNR detection, this peak tends to occur close to the merger time. In practice (especially at low SNR) the maximum can be influenced by noise, so this marker should be interpreted as a *useful visual reference*, not a guaranteed physical arrival time.  \n",
    "   Still, comparing the peak times across detectors can provide intuition about relative delays (which, physically, are on the order of milliseconds).\n",
    "\n",
    "2) **Frequency of maximum power**  \n",
    "   We estimate the detector’s frequency content using a power spectral density (PSD) estimate. The **dominant frequency** is taken as the frequency bin with maximum PSD value in a chosen band (e.g., 8–24 Hz).\n",
    "\n",
    "3) **Bandwidth around the dominant frequency**  \n",
    "   We first create a spectrogram of the signal and use it tocompute a power-containment bandwidth. First, we sum the spectrogram power over time to obtain a single “power vs frequency” profile. We then normalise this profile so it behaves like a probability distribution across frequency, and select the central frequency interval [f_\\min, f_\\max] that contains a chosen fraction of the total power (90% by default). The bandwidth is reported as $f_{max}$ - $f_{min}$, giving a compact measure of how concentrated the energy is in frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time @ peak power\n",
    "def time_peak_power(sample):\n",
    "    instants = np.zeros(3)\n",
    "    for det in range(len(sample)):\n",
    "        instants[det] += np.argmax(np.abs(sample[det]))\n",
    "    \n",
    "    return instants\n",
    "print(time_peak_power(sample))\n",
    "# frequency @ peak power\n",
    "def freq_peak_power(sample, fs=FS, nperseg=256, noverlap=192):\n",
    "    frequencies = np.zeros(3)\n",
    "    for det in range(3):\n",
    "        # compute spectrogram to get frequency information\n",
    "        f, t, Sxx = spectrogram(sample[det], fs=fs, nperseg=nperseg, \n",
    "                               noverlap=noverlap, scaling=\"density\")\n",
    "        \n",
    "        # find frequency with maximum power (sum across all times)\n",
    "        total_power_per_freq = Sxx.sum(axis=1)\n",
    "        peak_freq_idx = np.argmax(total_power_per_freq)\n",
    "        frequencies[det] = f[peak_freq_idx]\n",
    "    \n",
    "    return frequencies\n",
    "print(freq_peak_power(sample))\n",
    "\n",
    "def calculate_bandwidth(sample, power_threshold=0.99):\n",
    "    \"\"\"\n",
    "    Calculate frequency bandwidth containing specified power fraction\n",
    "    \"\"\"\n",
    "    bandwidths = np.zeros(3)\n",
    "    min_freqs = np.zeros(3)\n",
    "    max_freqs = np.zeros(3)\n",
    "    \n",
    "    for det in range(3):\n",
    "        f, t, Sxx = spectrogram(sample[det], fs=FS, nperseg=256, noverlap=192)\n",
    "        power_per_freq = Sxx.sum(axis=1)\n",
    "        \n",
    "        # normalize to create probability distribution\n",
    "        power_normalized = power_per_freq / power_per_freq.sum()\n",
    "        \n",
    "        # sort by frequency (not power) for cumulative calculation\n",
    "        sort_idx = np.argsort(f)\n",
    "        f_sorted = f[sort_idx]\n",
    "        power_sorted = power_normalized[sort_idx]\n",
    "        \n",
    "        # cumulative distribution\n",
    "        cumulative = np.cumsum(power_sorted)\n",
    "        \n",
    "        # find frequencies containing the threshold power\n",
    "        low_idx = np.argmax(cumulative >= (1 - power_threshold)/2)\n",
    "        high_idx = np.argmax(cumulative >= (1 + power_threshold)/2)\n",
    "        \n",
    "        bandwidths[det] = f_sorted[high_idx] - f_sorted[low_idx]\n",
    "        min_freqs[det] = f_sorted[low_idx]\n",
    "        max_freqs[det] = f_sorted[high_idx]\n",
    "    \n",
    "    return bandwidths, min_freqs, max_freqs\n",
    "bandwidths, min_freqs, max_freqs = calculate_bandwidth(sample)\n",
    "for det in range(3):\n",
    "    print(f\"{DETECTORS[det]}: Bandwidth = {bandwidths[det]:.1f} Hz ({min_freqs[det]:.1f}-{max_freqs[det]:.1f} Hz)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize time series evolution\n",
    "\n",
    "def plot_timeseries(sample, sample_id, target):\n",
    "    # get peak information\n",
    "    peak_times_idx = time_peak_power(sample)  \n",
    "    peak_times = peak_times_idx / FS            \n",
    "    peak_freqs = freq_peak_power(sample)           \n",
    "    bandwidth = calculate_bandwidth(sample)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.subplot(3, 1, i+1)\n",
    "        plt.plot(T, sample[i], linewidth=0.8)\n",
    "        \n",
    "        # get the actual peak value at that time\n",
    "        peak_power = sample[i, int(peak_times_idx[i])]\n",
    "        \n",
    "        # mark the peak point with a red dot\n",
    "        plt.scatter(peak_times[i], peak_power, color='red', s=50, zorder=5)\n",
    "        \n",
    "        plt.text(0.95, 0.95,\n",
    "                f'PEAK POWER\\nt = {peak_times[i]:.3f}s\\nf = {peak_freqs[i]:.1f}Hz\\nBandwidth:\\n {bandwidth[0][i]}Hz ({bandwidth[1][i]}Hz - {bandwidth[2][i]}Hz)',\n",
    "                transform=plt.gca().transAxes,  # use axis coordinates (0-1)\n",
    "                bbox=dict(boxstyle=\"round,pad=0.7\", facecolor=\"white\", alpha=0.8),\n",
    "                fontsize=9,\n",
    "                ha='right',\n",
    "                va='top')\n",
    "        \n",
    "        plt.title(f\"{DETECTORS[i]} - ID: {sample_id} - Target: {target}\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Strain\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "target = train_labels_df.iloc[0]['target']\n",
    "plot_timeseries(sample, sample_id, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrograms\n",
    "\n",
    "def plot_spectrogram(x, det=0, fs=FS, nperseg=256, noverlap=192, fmax=512, title=None):\n",
    "    \"\"\"\n",
    "    x: np.ndarray shape (3, 4096) or (4096,)\n",
    "    det: detector index if x is (3, 4096)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "    \n",
    "    for det in range(3):\n",
    "        sig = x[det]\n",
    "        f, t, Sxx = spectrogram(sig, fs=fs, nperseg=nperseg, noverlap=noverlap, \n",
    "                               scaling=\"density\", mode=\"magnitude\")\n",
    "        if fmax is not None:\n",
    "            m = f <= fmax\n",
    "            f, Sxx = f[m], Sxx[m, :]\n",
    "\n",
    "        axes[det].pcolormesh(t, f, 10*np.log10(Sxx + 1e-12), shading='gouraud')\n",
    "        axes[det].set_ylabel(\"Freq [Hz]\")\n",
    "        axes[det].set_title(DETECTORS[det])\n",
    "    \n",
    "    axes[2].set_xlabel(\"Time [s]\")\n",
    "    fig.suptitle(title or \"Spectrograms - All Detectors\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_spectrogram(sample, title=f\"ID: {sample_id} | Target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore random samples\n",
    "\n",
    "import random\n",
    "\n",
    "def random_sample_vis():\n",
    "    row = train_labels_df.sample(1).iloc[0]\n",
    "    sample_id = row.id\n",
    "    target = row.target\n",
    "    data = load_sample(sample_id)\n",
    "    \n",
    "    print(f\"Sample: {sample_id}, Target: {target}\")\n",
    "    plot_timeseries(data, sample_id, target)\n",
    "    plot_spectrogram(sample, det=det, title=f\"ID: {sample_id} | Target: {target} | {DETECTORS[det]}\")\n",
    "\n",
    "random_sample_vis()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
